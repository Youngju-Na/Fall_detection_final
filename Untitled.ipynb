{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading |"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fall Rate Identification, Detection, & Analysis (FRIDA) program\n",
    "For CSC 450 Course Project at Missouri State University\n",
    "Contributors: Jonah Falk, Samuel Pete, Normandy River, Niko Robbins, Jacob Schmoll\n",
    "License: GNU GPLv3\n",
    "Condensed-Space Model\n",
    "\"\"\"\n",
    "\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "# SRS: SIR.1, SIR.2, SIR.3\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from sklearn.utils.extmath import softmax\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Uncomment for debugging purposes:\n",
    "# print(\"opencv version:\", cv2.__version__)\n",
    "# print(\"onnx version:\", onnx.__version__)\n",
    "# print(\"onnxruntime version:\", ort.__version__)\n",
    "# print(\"torch version:\", torch.__version__)\n",
    "\n",
    "\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "class LoadModel:\n",
    "    def __init__(self):\n",
    "        self.onnx_model = onnx.load('model.onnx')\n",
    "        self.model = cv2.dnn.readNetFromONNX('model.onnx')\n",
    "        self.sess = ort.InferenceSession('model.onnx')\n",
    "        self.input_name = self.sess.get_inputs()[0].name\n",
    "\n",
    "\n",
    "# (Video Option 1) Use this for live video feed via a webcam.\n",
    "# Press 'q' to terminate.\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "# SRS: 3.1.3, UI.2\n",
    "class CameraSetUpLiveVideo:\n",
    "    def __init__(self, port):\n",
    "        self.cameraPort = port\n",
    "        self.camera = cv2.VideoCapture(self.cameraPort)\n",
    "        self.camera.set(cv2.CAP_PROP_FPS, 10)  # Sets frames per second (FPS).\n",
    "        video_brightness = 150\n",
    "        self.camera.set(10, video_brightness)\n",
    "        time.sleep(1)  # Gives the camera's auto-focus & auto-saturation time to load.\n",
    "\n",
    "\n",
    "# (Video Option 2) Use this for video file playback.\n",
    "# Video files will terminate once finished.\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "# SRS: 3.1.20, UI.2\n",
    "class CameraSetUpVideoPlayBack:\n",
    "    def __init__(self, path_to_video):\n",
    "        self.path_to_video = path_to_video\n",
    "        self.camera = cv2.VideoCapture(self.path_to_video)\n",
    "        self.camera.set(cv2.CAP_PROP_FPS, 32)  # Set frames per second (FPS).\n",
    "        video_brightness = 150\n",
    "        self.camera.set(10, video_brightness)\n",
    "        time.sleep(1)  # Gives the camera's auto-focus & auto-saturation time to load.\n",
    "\n",
    "\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "# SRS: 3.2.13\n",
    "class TransformShape:\n",
    "    def __init__(self, frame):\n",
    "        self.frame = frame\n",
    "        self.frame_transform = np.zeros_like(self.frame)\n",
    "\n",
    "\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "# SRS: 3.2.13\n",
    "class MotionHistoryTransform:\n",
    "    dim = (224, 256)\n",
    "\n",
    "    def __init__(self, frame):\n",
    "        self.frame = frame\n",
    "        self.dims = (256, 224, 3)\n",
    "        self.dim = (224, 256)\n",
    "        self.mhi_zeros = np.zeros(self.dims)\n",
    "        self.prev_frame = cv2.resize(self.frame, self.dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "# SRS: 3.2.13\n",
    "class MotionHistoryDifference:\n",
    "    def __init__(self, frame):\n",
    "        self.frame = frame\n",
    "        self.resized = cv2.resize(self.frame, MotionHistoryTransform.dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "# SDD: 2.2, 3.0, 3.1, 3.2, 3.2.1, 3.2.3, 3.2.3.5, 3.2.3.5.1, 3.3\n",
    "# SRS: 3.2.13\n",
    "class CreateBatch:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.batch = []\n",
    "        self.condense_batch = []\n",
    "\n",
    "\n",
    "# SRS: CIR.1\n",
    "def animate():\n",
    "    for c in itertools.cycle(['|', '/', '-', '\\\\']):\n",
    "        if done:\n",
    "            break\n",
    "        sys.stdout.write('\\rLoading ' + c)\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(0.1)\n",
    "    sys.stdout.write('\\rFRIDA software loaded.\\n')\n",
    "\n",
    "\n",
    "done = False  # Program hasn't yet loaded video.\n",
    "t = threading.Thread(target=animate)\n",
    "t.start()\n",
    "\n",
    "condensedSpacedModel = LoadModel()\n",
    "batchCreate = CreateBatch()\n",
    "\n",
    "# (Video Option 1) Use this class for live video feed.\n",
    "# 0 = system's default webcam (recommended), 1 = external webcam, -1 = auto-detection\n",
    "# Only change argument for debugging purposes.\n",
    "camera = CameraSetUpLiveVideo(0)\n",
    "\n",
    "# (Video Option 2) Use this class for video file playback.\n",
    "# Change \"fallcam0/fall1cam0.mp4\" to the video file of your choice.\n",
    "# Refer to the adl, fallcam0, & fallcam1 dataset folders.\n",
    "# camera = CameraSetUpVideoPlayBack(\"fallcam0/fall1cam0.mp4\")\n",
    "\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "countFrame = 0\n",
    "HUD = 1  # Heads Up Display for live video feed. Set to 0 to turn off.\n",
    "mhi_maker = None\n",
    "prev_mhi = None\n",
    "\n",
    "# SRS: 3.1.3, 3.1.20, CIR.1\n",
    "if not camera.camera.isOpened():\n",
    "    raise IOError(\"CANNOT LOAD VIDEO FRAME\")\n",
    "\n",
    "while True:\n",
    "    # SRS: 3.1.3, 3.1.20\n",
    "    grabbed, frame = camera.camera.read()\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        countFrame += 1\n",
    "        frameTransform = TransformShape(frame)\n",
    "        # SDD: 3.2.3.5, 3.2.3.5.1\n",
    "        frame = cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2GRAY)\n",
    "        frameTransform.frame_transform[:, :, 0] = frame\n",
    "        frameTransform.frame_transform[:, :, 1] = frame\n",
    "        frameTransform.frame_transform[:, :, 2] = frame\n",
    "        frame = frameTransform.frame_transform\n",
    "\n",
    "        # Appends the current frame to the next frame.\n",
    "        if countFrame == 1:\n",
    "            mhi_maker = MotionHistoryTransform(frame)\n",
    "            prev_mhi = mhi_maker.mhi_zeros\n",
    "\n",
    "        else:\n",
    "            mhi_difference_maker = MotionHistoryDifference(frame)\n",
    "            # SDD: 3.2.3.5, 3.2.3.5.1\n",
    "            diff = cv2.absdiff(mhi_maker.prev_frame, mhi_difference_maker.resized)\n",
    "            binary = (diff >= (.41 * 255)).astype(np.uint8)\n",
    "            mhi = binary + (binary == 0) * np.maximum(mhi_maker.mhi_zeros, (prev_mhi - 1 / 16))\n",
    "            mhi_maker.prev_frame = mhi_difference_maker.resized\n",
    "            prev_mhi = mhi\n",
    "            frameTransform.frame_transform = mhi\n",
    "\n",
    "        frameTransform.frame_transform = cv2.resize(frameTransform.frame_transform,\n",
    "                                                    MotionHistoryTransform.dim, interpolation=cv2.INTER_AREA)\n",
    "        frames = np.expand_dims(frameTransform.frame_transform, axis=0)\n",
    "        frames = np.array(frames)\n",
    "        frames = frames.astype(numpy.float32)\n",
    "        image = torch.from_numpy(frames)\n",
    "        image = image.permute(0, 3, 1, 2)\n",
    "        detectStatus = \"Idle\"\n",
    "\n",
    "        if countFrame == 1:\n",
    "            result = np.array(image)\n",
    "\n",
    "        elif countFrame > 1 and countFrame % 2 == 0:\n",
    "            result = np.array(image)\n",
    "\n",
    "        condense = np.array(image)\n",
    "\n",
    "        if len(batchCreate.batch) != 32:\n",
    "            batchCreate.batch.append(result)\n",
    "            batchCreate.condense_batch.append(condense)\n",
    "\n",
    "        if len(batchCreate.batch) > 32:\n",
    "            batchCreate.batch = batchCreate.batch[:32]\n",
    "            batchCreate.batch = np.array(batchCreate.batch)\n",
    "\n",
    "        if len(batchCreate.batch) == 32:\n",
    "            result_x = np.concatenate(batchCreate.batch, axis=0)\n",
    "            model_input_x = result_x\n",
    "            res = condensedSpacedModel.sess.run(None, {condensedSpacedModel.input_name: model_input_x})\n",
    "            norm = softmax(res[0])\n",
    "\n",
    "            for x in norm:\n",
    "                fall = x.item(0)\n",
    "                notFall = x.item(1)\n",
    "                # SRS: CIR.1\n",
    "                # FP = Fall Prediction, NFP = Non-Fall Prediction\n",
    "                print(\"FP\", \"{0:.2%}\".format(fall), \"NFP\", \"{0:.2%}\".format(notFall))\n",
    "                # SRS: 3.1.6, 3.1.22, CIR.1, CIR.2\n",
    "                if fall > notFall:\n",
    "                    detectStatus = \"FALL DETECTED\"\n",
    "                    print(detectStatus)\n",
    "                    batchCreate.batch = []\n",
    "                    break\n",
    "\n",
    "            # Appends the current frame to the next frame.\n",
    "            batchCreate.batch = batchCreate.condense_batch[16::]\n",
    "            batchCreate.condense_batch = []\n",
    "\n",
    "        # SRS: CIR.2\n",
    "        if HUD:\n",
    "            if detectStatus == \"FALL DETECTED\":\n",
    "                cv2.putText(frame, \"Status: {}\".format(detectStatus),\n",
    "                            (10, 20), cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 0, 255), 1)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Status: {}\".format(detectStatus),\n",
    "                            (10, 20), cv2.FONT_HERSHEY_DUPLEX, 0.8, (0, 128, 0), 1)\n",
    "\n",
    "        # (Default) Loads video frame window in grayscale.\n",
    "        # SDD: 3.2.3.5, 3.2.3.5.1\n",
    "        # SRS: UI.2\n",
    "        cv2.imshow(\"Video Feed\", frame)\n",
    "\n",
    "        # (Optional) Loads video frame window using background subtraction.\n",
    "        # SDD: 3.2.3.5, 3.2.3.5.1\n",
    "        # SRS: UI.2\n",
    "        # cv2.imshow(\"Background Subtraction\", frameTransform.frame_transform)\n",
    "\n",
    "        done = True  # Video has successfully loaded.\n",
    "\n",
    "        # SDD: 3.2.3.5, 3.2.3.5.1\n",
    "        # SRS: CIR.1, CIR.3\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print(\"\\nVIDEO FEED TERMINATED\\n\")\n",
    "            camera.camera.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
